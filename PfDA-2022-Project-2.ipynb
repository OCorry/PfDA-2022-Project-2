{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6321591",
   "metadata": {},
   "source": [
    " # <font color=darkblue>Programming for Data Analysis 2022 - Project 2</font>\n",
    "\n",
    "## <font color=darkblue>Orla Corry</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a3b53",
   "metadata": {},
   "source": [
    "## <font color=darkblue>Table of Contents</font>\n",
    "***\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "\n",
    "\n",
    "- [Importing Libraries](#Importing-Libraries)\n",
    "\n",
    "\n",
    "- [Reading in the csv file](#Reading-in-the-csv-file)\n",
    "\n",
    "\n",
    "- [Analysis/review of the dataset](#Analysis/review-of-the-dataset)\n",
    "\n",
    "\n",
    "- [Literature review on classifiers](#Literature-review-on-classifiers)\n",
    "\n",
    "\n",
    "- [Statistical Analysis of the Dataset](#Statistical-Analysis-of-the-Dataset)\n",
    "\n",
    "\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bbe6f",
   "metadata": {},
   "source": [
    "## <font color=darkblue>Introduction</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d31cd3c",
   "metadata": {},
   "source": [
    "## <font color=darkblue>Importing Libraries</font>\n",
    "***\n",
    "\n",
    "For this project I will be importing the following Libraries:\n",
    "\n",
    "1. **pandas** - To read in the dataset and to analyse data that is in tabular form\n",
    "\n",
    "\n",
    "2. **matplotlib.pyplot** - For Plotting data \n",
    "\n",
    "\n",
    "3. **seaborn** - For Plotting data\n",
    "\n",
    "\n",
    "4. **sklearn** - For regression analysis \n",
    "\n",
    "\n",
    "5. **numpy** - For working with arrays and synthasising data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b311a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn import linear_model \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d017722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table{float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table{float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc9b9d",
   "metadata": {},
   "source": [
    "## <font color=darkblue>Reading in the csv file</font>\n",
    "***\n",
    "I have sourced the Wesconsin Breast Cancer Dataset from <a href=\"https://github.com/jeffheaton/aifh/blob/master/vol1/python-examples/datasets/breast-cancer-wisconsin.csv\" target=\"_top\">Heaton. J (Github), 2013</a>. To pull the actual csv URL from Heaton's Github, I clicked on the *Raw* tab and copied & pasted the URL that was provided for the raw data. Below, I have read in the the csv file to Python using the built in library <a href=\"https://pandas.pydata.org/docs/\" target=\"_top\">Pandas</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83711859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the csv file using pandas\n",
    "# Using dataset from https://github.com/jeffheaton/aifh/blob/master/vol1/python-examples/datasets/breast-cancer-wisconsin.csv\n",
    "dataset = (\"https://raw.githubusercontent.com/jeffheaton/aifh/master/vol1/python-examples/datasets/breast-cancer-wisconsin.csv\")\n",
    "df = pd.read_csv (dataset) #storing the read in csv dataframe as df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a62093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>size_uniformity</th>\n",
       "      <th>shape_uniformity</th>\n",
       "      <th>marginal_adhesion</th>\n",
       "      <th>epithelial_size</th>\n",
       "      <th>bare_nucleoli</th>\n",
       "      <th>bland_chromatin</th>\n",
       "      <th>normal_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  clump_thickness  size_uniformity  shape_uniformity  \\\n",
       "0  1000025                5                1                 1   \n",
       "1  1002945                5                4                 4   \n",
       "2  1015425                3                1                 1   \n",
       "3  1016277                6                8                 8   \n",
       "4  1017023                4                1                 1   \n",
       "\n",
       "   marginal_adhesion  epithelial_size bare_nucleoli  bland_chromatin  \\\n",
       "0                  1                2             1                3   \n",
       "1                  5                7            10                3   \n",
       "2                  1                2             2                3   \n",
       "3                  1                3             4                3   \n",
       "4                  3                2             1                3   \n",
       "\n",
       "   normal_nucleoli  mitoses  class  \n",
       "0                1        1      2  \n",
       "1                2        1      2  \n",
       "2                1        1      2  \n",
       "3                7        1      2  \n",
       "4                1        1      2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the dataframe (df) to make sure the csv file read in to Python properly\n",
    "df.head()\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2718bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df['bare_nucleoli'] = df['bare_nucleoli'].replace(['?'], ['NaN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22a7b8",
   "metadata": {},
   "source": [
    "## <font color=darkblue>Analysis/review of the dataset</font>\n",
    "***\n",
    "The Breast Cancer Wisconsin (Original) Data Set is a classification dataset that records measurements under a number of attributes <a href=\"http://odds.cs.stonybrook.edu/breast-cancer-wisconsin-original-dataset/\" target=\"_top\">ODDS. (2022)</a> in order to predict whether a case is either of class benign or class malignant. These attributes will be discussed in more detail later on.\n",
    "\n",
    "Using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html\" target=\"_top\">df.shape</a> command on the dataframe above, we can see that the dataset comprises of 699 instances/cases (rows) of data across 11 attributes (columns). \n",
    "\n",
    "The 699 rows of data were gathered over a period of time which spanned from January 1989 through to November 1991. The data gathered over the 8 periods during this time is organised into 8 groups. \n",
    "\n",
    "There are 11 features or attributes to this dataset, all of which are of data type *integer*. Starting at column index 0 through to index 10, the first column contains the *id* of the case/instance which are integers with multiple digits. From column index 1 to index 9 the 9 attributes, namely; *clump thickness*, *size uniformity*, *shape uniformity*, *marginal adhesion*, *epithelial size*, *bare nucleoli*, *bland chromatin*, *normal nucleoli* and *mitoses* are given values ranging from 1 to 10. According to <a href=\"https://www.journalbinet.com/uploads/2/1/0/0/21005390/67.02.09.2020_analysis_of_wisconsin_breast_cancer_original_dataset_using_data_mining_and_machine_learning_algorithms_for_breast_cancer_prediction.pdf\" target=\"_top\">Ahmed, T. et al. (2020)</a> the larger the value for these 9 attruibutes, the greater the chance of the case being malignant. Finally, the last column contains the attribute *class* which contains values either 2 or 4 where 2 represents benign and 4 represents malignant <a href=\"https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)\" target=\"_top\">Wolberg, W.H. (1992)</a>. **Table 1.** below summarises all of the attributes (excluding the *id** attribute) and what values they can take.\n",
    "\n",
    "<ins>**Table 1.**</ins>\n",
    "\n",
    "\n",
    "\n",
    "|*Attributes*      |*Value*|\n",
    "|:-----------------|:--:\n",
    "|\n",
    "|Clump thickness   |1-10|\n",
    "|Size uniformity   |1-10| \n",
    "|Shape uniformity  |1-10|\n",
    "|Marginal Adheasion|1-10|\n",
    "|Epithelial size   |1-10| \n",
    "|Bare nucleoli     |1-10|\n",
    "|Bland chromatin   |1-10| \n",
    "|Normal nucleoli   |1-10| \n",
    "|Mitoses           |1-10|\n",
    "|Class             |2 = benign & 4= malignant|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfef881a",
   "metadata": {},
   "source": [
    "## <font color=darkblue>Literature review on classifiers</font>\n",
    "***\n",
    "While there are may different variations of definitions of classifiers in machine learning, according to the <a href=\"https://scikit-learn.org/stable/\" target=\"_top\">scikit-learn documentation (2022)</a>, classifiers identify which category an object belongs to. A classifier can be defined as an algorithm that automatically assigns data points to a range of categories or classes and there are two main models; supervised and unsupervised <a href=\"https://www.indeed.com/career-advice/career-development/classifiers-in-machine-learning\" target=\"_top\">Indeed Editorial Team. (2022)</a>.\n",
    "\n",
    "This section sets out to research some previous studies done using the Wisconsin Breast Cancer (Original) Data Set, and to establish what classifiers were used and compare their performances. \n",
    "\n",
    "### <font color=darkblue><ins>Study 1</ins></font>\n",
    "\n",
    "##### <ins>Introduction</ins>\n",
    "A study titled *Study and Analysis of Breast Cancer Data* carried out by <a href=\"https://www.ijert.org/research/study-and-analysis-of-breast-cancer-data-IJERTCONV5IS21015.pdf\" target=\"_top\">Manoli. N.S and Padma. S.K (2017) </a> used two forms of classifiers namely; the Naive-Bayes Classifier and the Support Vector Machine (SVM) classifier along with a further feature called **<a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\" target=\"_top\">Principal Component Analysis (PCA) </a>**. A PCA is a machine learning technique used on datasets with many variables/features to reduce the dimensions of the dataset. This is used because if a dataset has a lot of input attributes, it can lead to the classifier failing <a href=\"https://www.upgrad.com/blog/pca-in-machine-learning/\" target=\"_top\">Vadapalli. P. (2020) </a>.\n",
    "\n",
    "\n",
    "##### <ins>Detail of Classifiers Used</ins>\n",
    "\n",
    "Firstly, Niave-Bayes is a classifier which gives a conditional probability of one event occuring, given another event <a href=\"https://www.simplilearn.com/tutorials/machine-learning-tutorial/naive-bayes-classifier#:~:text=how%20it%20works.-,Understanding%20Naive%20Bayes%20Classifier,event%20A%20given%20event%20B.\" target=\"_top\">Banoula. M. (2022) </a>\n",
    "\n",
    "According to the authors, Naive-Bayes is an effective classifier to use for this study as it is a practical classifier for real and discrete data. As this data set contains all real and discrete data, then Naive-Bayes is a good choice for the study.  \n",
    "\n",
    "\n",
    "The second classifier used, Support Vector Machine (SVM) is a classifier that takes the data as an input and outputs a line or \n",
    "<a href=\"https://www.analyticsvidhya.com/blog/2021/03/beginners-guide-to-support-vector-machine-svm/#:~:text=A%20hyperplane%20is%20a%20decision,input%20features%20in%20the%20dataset.\" target=\"_top\">hyperplane</a> that separates the data into classes <a href=\"https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989\" target=\"_top\">Pupale. R. (2018)</a>.\n",
    "\n",
    "Again, for this study, SVM is effective as it can seperate the input data into 2 Classes, Benign and Malignant. \n",
    "\n",
    "\n",
    "##### <ins>Training</ins>\n",
    "For this study, the dataset was divided 80/20 where 80% of the dataset was used training data and 20% was used as test data. This was carried out 10 times, testing 20% random data from the dataset and then taking the average accuracy from the 10 iterations.\n",
    "\n",
    "##### <ins>Findings of the study</ins>\n",
    "This Manoli. N.S & Padma. S.K (2017) study found that both of the classifiers were very accurate as they both had accuracy rates above 95%. The Naive Bayes Classifier scored an accuracy rate of 95.71% for the whole data (data where PCA was not applied) and 97.14% accuracy where the PCA feature was applied. However, the Support Vector Machine (SVM) was a little superior with a accuracy result of 97.14% for the whole data (data where PCA was not applied) and 97.86% accuracy where the PCA feature was applied. \n",
    "\n",
    "### <font color=darkblue><ins>Study 2</ins></font>\n",
    "##### <ins>Introduction</ins>\n",
    "<a href=\"https://www.journalbinet.com/uploads/2/1/0/0/21005390/67.02.09.2020_analysis_of_wisconsin_breast_cancer_original_dataset_using_data_mining_and_machine_learning_algorithms_for_breast_cancer_prediction.pdf\" target=\"_top\">Ahmed. T. et al (2020)</a>\n",
    " carried out a study titled *Analysis of Wisconsin Breast Cancer original dataset using data mining and machine learning algorithms for breast cancer prediction* on a number of different classifiers. Again, Naïve Bayes was one of their chosen classifiers. Others to note were Multilayer perceptron (MLP), J48 and Support vector machine (SVM).\n",
    "\n",
    "##### <ins>Detail of Classifiers Used</ins>\n",
    "\n",
    "In addition to the Naive-Bayes and (NB) and Support vector machine (SVM) classifiers already discussed under study 1 above, this study also looks at two more classifiers; Multilayer perceptron (MLP) and J48.\n",
    "\n",
    "MLP is a neural network which has three layers; an input layer, a hidden layer and an output layer. The data is transmitted from the input layer through the hidden layer (where the computations and operations are carried out on the data) to the output layer <a href=\"https://www.educative.io/answers/what-is-a-multi-layered-perceptron\" target=\"_top\">Educative, Inc (2022)</a>.\n",
    "\n",
    "J48, then is a algorithm that builds decision trees based  on a set of training data by using information entropy <a href=\"https://medium.com/@nilimakhanna1/j48-classification-c4-5-algorithm-in-a-nutshell-24c50d20658e\" target=\"_top\">Khanna.N. (2021)</a>.\n",
    "\n",
    "##### <ins>Training</ins>\n",
    "\n",
    "Initially, for this study, the authors used the full dataset for training and carried this out ten times. However, when working in more depth with the Niave-Bayes classifier, they carried out multiple different tests some using test/train split. \n",
    "\n",
    "##### <ins>Findings of the study</ins>\n",
    "The study found the Naïve Bayes to be the highest performing classifier with an accuracy rate of 97.2779%. Following this was the Multilayer Perceptron (MLP) Classifer with an accuracy rate of 96.1318%. The lowest scoring classifier in this study was the J48 classifier at 94.2693%. \n",
    "\n",
    "When they dug deeper into the Naïve Bayes classifier, they carried out different trials by removing a single feature (or attribute) at a time to establish how each attribute effected the classifiers performance. They found that by removing the *Single Epithelial Cell Size* attribute from the dataset, the classifier was most accurate. Their most accurate result came about by splitting the data into 85.5% train data and 14.65% test data. The outcome of this was that the classifier was 99.0099% accurate. \n",
    "\n",
    "Again, this study reveals that the Naive Bayes classifier is most effective and accuarate classifier from the the classifiers they studied. One thing to note here is that in this study the Principal Component Analysis (PCA) technique was not used. However, in their conclusion, they did refer to using PCA in future studies. Incorporating the PCA may have an impact on the accuracy of the Naive Bayes classifier.\n",
    "\n",
    "\n",
    "### <font color=darkblue><ins>Study 3</ins></font>\n",
    "##### <ins>Introduction</ins>\n",
    "<a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6408508\" target=\"_top\">G. Salama et al. (2012)</a> published a paper titled *Experimental Comparison of Classifiers for Breast Cancer Diagnosis* in which they investigated the performance of a number of classifiers. They focused on decision tree (J48), Multi-LayerPerception (MLP), Naive Bayes (NB), Sequential Minimal Optimization (SMO), and Instance Based for K-Nearest neighbor (IBK). \n",
    "\n",
    "\n",
    "##### <ins>Detail of Classifiers Used</ins>\n",
    "Again, MLP, NB and J48 have already been explained above. However, this study worked with the following two additional classifiers: \n",
    "\n",
    "Sequential Minimal Optimization (SMO), is an algorithm used when training SVM algorithm <a href=\"https://en.wikipedia.org/wiki/Sequential_minimal_optimization\" target=\"_top\">Wikipedia. (2022)</a>. \n",
    "\n",
    "Instance Based for K-Nearest neighbor (IBK) is a classifier which predicts the outcome based on the k number of nearest neighbours of the data that has been inputted. \n",
    "\n",
    "<ins>Training</ins>\n",
    "The authors of this paper do not give detail of what training/testing data ratio they used for the classifiers. However, as well as looking that the classifiers on an individual basis, they also have looked at merging or *fusing* classifiers together to obtain more accurate results. \n",
    "\n",
    "##### <ins>Findings of the study</ins>\n",
    "By using a single classifier on it's own, they found that Sequential Minimal Optimization (SMO) classifier was the most accurate at an accuracy rate of 96.9957%, followed by the Naive Bayes (NB) classifier at 95.9943% accuracy. The decision tree (J48) was the least accurate of the classifiers tested at an accuracy of 95.1359%. \n",
    "\n",
    "However, they delved into their accuracy more by *fusing* or combining the classifiers together to create *multi-classifiers*. They found that by fusing SMO, IBK, NB and J48 classifiers, the overall accuracy was 97.2818% which was more accurate than using any of the classifiers individually. \n",
    "\n",
    "Furthermore, by implementing Principal Component Analysis (PCA) to combine related attributes and consequently reduce the number of input variables, they found their most accurate multi-classifier of all. This was a fusion of the J48 and MLP classifiers. This is interesting because originally, using the classifiers individually, the J48 and MLP were the two least accurate classifiers of those examined in the study. \n",
    "\n",
    "\n",
    "### <font color=darkblue><ins>Study 4</ins></font>\n",
    "##### <ins>Introduction</ins>\n",
    "A paper titled *Performance Analysis on Three Breast Cancer Datasets using Ensemble Classifiers Techniques* by <a href=\"file:///C:/Users/Admin/Downloads/kkuenjadmin,+8-23-20.pdf\" target=\"_top\">Arach. S. and Bouden. H. (2019)</a> also looked into classifiers used on the Wisconsin Breast Cancer Data Set. They looked into the accuracy of six classifiers; Bayesian networks (BN), Multi-LayerPerception (MLP), decision tree (J48), Sequential Minimal Optimization (SMO), Random Forest (RF) and \n",
    "Instance Based for K-Nearest neighbor (IBK).\n",
    "\n",
    "##### <ins>Detail of Classifiers Used</ins>\n",
    "The only new classifier in this study that we have not come across before is the Random Forest (RF) algorithm. Again similar to that of the J48 algorithm, RN is a algorithm that has numerous decision trees. The output generated is the class that has been selected by the majority of the decision trees <a href=\"https://en.wikipedia.org/wiki/Random_forest\" target=\"_top\">Wikipedia (2022)</a> \n",
    "\n",
    "\n",
    "<ins>Training</ins>\n",
    "Training and testing on the classifiers was based on cross validation of 10-fold as a method of testing. *Cross validation of 10-fold*  means that the dataset is divided randomly into 10 parts. Nine of the parts are used for training and one part is used for testing. The process is then repeated 10 times so a different tenth is tested <a href=\"https://www.kdnuggets.com/2018/01/training-test-sets-cross-validation.html\" target=\"_top\">Zacharski. R. (2022)</a>  \n",
    "\n",
    "In their study, the authors also incorporated a feature selection method called **<a href=\"https://www.scikit-yb.org/en/latest/api/model_selection/rfecv.html\" target=\"_top\">Recursive Feature Elimination (RFE)</a>** which eliminates the least significant features (attributes) until the correct number of features is reached <a href=\"https://www.scikit-yb.org/en/latest/api/model_selection/rfecv.html\" target=\"_top\">The scikit-yb developers (2019)</a>. \n",
    "\n",
    "##### <ins>Findings of the study</ins>\n",
    "The results of the study show that used individually, the Baysian Networks (BN) classifier was the most accurate at an accuracy rate of 97.28%. The least accurate classifier was the decision tree (J48) at 95.15% accuracy. \n",
    "Combining (BN) with other classifiers gave interesting results. Combining BN with SMO resulted in a decrease in the accuracy, and the similarly fusing BN with MLP reduced the accuracy also. Combining 4 classifiers; BN RF SMO and IBK increased accuracy to 97.99%. These tests also included the Principal Component Analysis (PCA).\n",
    "\n",
    "Their study found that the removal of the Recursive Feature Elimination (RFE), feature selection method reduced the accuracy of the classifiers across the board, both individually and as multi-classifiers.\n",
    "\n",
    "\n",
    "### <font color=darkblue><ins>Study 5</ins></font>\n",
    "##### <ins>Introduction</ins>\n",
    "<a href=\"https://ijnaa.semnan.ac.ir/article_5965_a1ca78998e8760f2915afc8b1d037e64.pdf\" target=\"_top\">Al-Joda. A.A, et al. (2021).</a> comprised a study called *Comparison of classification techniques based on medical datasets* in which they focused on three classifiers; Support Vector Machine (SVM), Adaptive Boosting (AdaBoost) and  Random forests (RF). They used these classifiers on two datasets, one of which being the Wisconsin Breast Cancer Data Set.\n",
    "\n",
    "##### <ins>Detail of Classifiers Used</ins>\n",
    "Al-Joda. A.A, et al. (2021) have used a classifier that we have not yet come across and this is the Adaptive Boosting (AdaBoost) algorithm. This is an algorithm that creates a stronger classifier from a number of weaker classifiers (mostly decision tree algorithms) <a href=\"https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/\" target=\"_top\">Brownlee. J. (2016).</a>\n",
    "\n",
    "\n",
    "##### <ins>Training</ins>\n",
    "Again, this study used the <a href=\"https://www.kdnuggets.com/2018/01/training-test-sets-cross-validation.html\" target=\"_top\">10 fold cross-validation</a> method for training and testing. \n",
    "\n",
    "\n",
    "##### <ins>Findings of the study</ins>\n",
    "Their findings showed that the Adaptive Boosting (AdaBoost) was the most accurate of the three classifiers used with an accuracy of 100%. The other two classifiers; Support Vector Machine (SVM) and Random forests (RF) were also found to be very accurate at a rate of 0.976% accuracy. \n",
    "\n",
    "\n",
    "### <font color=darkblue><ins>Study 6</ins></font>\n",
    "##### <ins>Introduction</ins>\n",
    "A study by <a href=\"file:///C:/Users/Admin/Downloads/monther,+12300-Article+Text-36825-1-6-20210211%20(1).pdf\" target=\"_top\">Abdulkareema S.A and Abdulkareem Z.O.(2021).</a> titled *An Evaluation of the Wisconsin Breast Cancer Dataset using Ensemble Classifiers and RFE Feature Selection Technique* focuses on two classifiers, namely; Random Forest (RF) and eXtreme Gradient Boost (XGBoost) along with the inclusion of a feature selection technique, Recursive Feature Elimination (RFE).\n",
    "\n",
    "##### <ins>Detail of Classifiers Used</ins>\n",
    "RF has already been explained above in Study 4. eXtreme Gradient Boost (XGBoost). Again this is a strong classifier generated from several weaker classifiers (mostly decision trees) <a href=\"https://www.mygreatlearning.com/blog/xgboost-algorithm/\" target=\"_top\">Great Learning Team.(2022).</a> \n",
    "\n",
    "##### <ins>Training</ins>\n",
    "For the study the authors split the data 80/20, where 80% of the dataset was training data and 20% of the data was testing data.\n",
    "\n",
    "##### <ins>Findings of the study</ins>\n",
    "Their work shows that again both of the classifiers are quite accurate with accuracy rates of 97.07% and 98.53% for the RF and XGBoost respectively. This part of the experiment did not include the feature selection technique and hence all of the features(attributes) were inlcuded in the calculation.\n",
    "\n",
    "However with the inclusion of the Recursive Feature Elimination feature selection technique, the results improved further. The accuracy of the RF classifier increased to 98.05% and the accuracy of the XGBoost increased to 99.02%. \n",
    "\n",
    "The authors provided a \n",
    "**<a href=\"https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/\" target=\"_top\">confusion matrix.</a>**\n",
    "which gives results of the XGBoost classifier and the RFE feature selection technique. This matrix shows that the XGBoost classifier along with the RFE technique predicted all of the malignant data correctly and was also very accuarte with the benign data, having predicted 2 cases to be malignant but were actually benign.\n",
    "\n",
    "\n",
    "### <font color=darkblue><ins>Study 7</ins></font>\n",
    "##### <ins>Introduction</ins>\n",
    "A study titled *Breast Cancer Classification Using Machine Learning* <a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8391453\" target=\"_top\">M. Amrane. et al. (2018)</a> focused on k-Nearest Neighbors (KNN) and Naive Bayes (NB) classifiers. \n",
    "\n",
    "##### <ins>Detail of Classifiers Used</ins>\n",
    "Explanations of k-Nearest Neighbors (KNN) and Naive Bayes (NB) have already been provided in Study 3 and Study 1 above.\n",
    "\n",
    "##### <ins>Training</ins>\n",
    "For the KNN classifier, there was no training stage as they predicted the outcome of a new instance  by calculating the <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\" target=\"_top\">Euclidean distance</a> between the instance and all of thr points on the training set. \n",
    "\n",
    "For the Naive Bayes algorithm, they divided the dataset into a training set and a test set. The training set was then divided into two further sets where *D* represents the presence of a tumor. *D* was then divided into two classes malignant(4) and benign(2). *T* is the test. \n",
    "\n",
    "This study did not include any feature selection technique or any other form of technique to cut down or couple up the features. \n",
    "\n",
    "##### <ins>Findings of the study</ins>\n",
    "Again this study was conclusive in its findings that both of the classifiers were effective and accurate. The KNN classifier was just a little ahead of the NB classifier with rates of 97.51% and 96.19% respectively. However, the authors did conclude that if the dataset had been bigger, KNN would not have been as efficient as the algorithm is more complex and would take longer to run. \n",
    "\n",
    "\n",
    "### <font color=darkblue><ins>Conclusion</ins></font>\n",
    "The above findings show that there are may different classifiers used in machine learning, most of which are quite accurate. \n",
    "\n",
    "    \n",
    "The most common classifier throughout the literature was the Naive Bayes algortithm which scored well in terms of accuracy across all of the studies that it was used in. Other strong classifiers to note are the Support Vector Machine (SVM) and the Multi-LayerPerception (MLP). \n",
    "    \n",
    "In study 3, the decision tree (J48) showed to be a little weaker in terms of accuaracy but when combined with other classifiers it contributed to make a stronger classifier over all. Also, the use of the Principal Component Analysis (PCA) made the J48 algorithm stronger. My research above from study 5 and study 6, also backs up that on it's own J48 is a weaker algortihm as the Adaptive Boosting (AdaBoost) Gradient Boost (XGBoost) are stronger classifiers which are made up of weaker classifiers such as decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d3de6c",
   "metadata": {},
   "source": [
    "## <font color=darkblue>Statistical Analysis of the Dataset</font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a87e47",
   "metadata": {},
   "source": [
    "## <font color=darkblue>References</font>\n",
    "***\n",
    "\n",
    "1. Heaton J. Github. breast-cancer-wisconsin.csv (2013) https://github.com/jeffheaton/aifh/blob/master/vol1/python-examples/datasets/breast-cancer-wisconsin.csv\n",
    "\n",
    "\n",
    "2. Pandas Documentation. (2022). https://pandas.pydata.org/docs/\n",
    "\n",
    "\n",
    "3. Pandas. pandas.DataFrame.shape. (2022). https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html\n",
    "\n",
    "\n",
    "4. ODDS. Breast Cancer Wisconsin (Original) dataset. (2022). http://odds.cs.stonybrook.edu/breast-cancer-wisconsin-original-dataset/\n",
    "\n",
    "\n",
    "5. Ahmed. T, Omtiaz. N and Karmakar. A.(2020). Analysis of Wisconsin Breast Cancer original dataset using data\n",
    "mining and machine learning algorithms for breast cancer\n",
    "prediction\n",
    "https://www.journalbinet.com/uploads/2/1/0/0/21005390/67.02.09.2020_analysis_of_wisconsin_breast_cancer_original_dataset_using_data_mining_and_machine_learning_algorithms_for_breast_cancer_prediction.pdf\n",
    "\n",
    "\n",
    "6. Wolberg, W. H. (1992) Breast Cancer Wisconsin (Original) Data Set. https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)\n",
    "\n",
    "\n",
    "7. How to align table in Jupyter Notebook. Deepsim Acadamey. (2021). https://www.youtube.com/watch?v=_1jg0FlzpUs\n",
    "\n",
    "\n",
    "8. scikit-learn. (2022). https://scikit-learn.org/stable/\n",
    "\n",
    "\n",
    "\n",
    "9. Indeed Editorial Team. Machine Learning Classifiers: Definition and 5 Types. (2022). https://www.indeed.com/career-advice/career-development/classifiers-in-machine-learning\n",
    "\n",
    "\n",
    "10. Manoli. N.S & Padma. S.K. Study and Analysis of Breast Cancer Data (2017) https://www.ijert.org/research/study-and-analysis-of-breast-cancer-data-IJERTCONV5IS21015.pdf\n",
    "\n",
    "\n",
    "11. Wikipedia. Principal component analysis. (2022). https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "\n",
    "\n",
    "12. Vadapalli. P. (2020). PCA in Machine Learning: Assumptions, Steps to Apply & Applications. https://www.upgrad.com/blog/pca-in-machine-learning/\n",
    "\n",
    "\n",
    "13. Banoula. M. Understanding Naive Bayes Classifier. (2022). https://www.simplilearn.com/tutorials/machine-learning-tutorial/naive-bayes-classifier#:~:text=how%20it%20works.-,Understanding%20Naive%20Bayes%20Classifier,event%20A%20given%20event%20B.\n",
    "\n",
    "\n",
    "14. Saxena. S. Beginner’s Guide to Support Vector Machine(SVM) (2021). https://www.analyticsvidhya.com/blog/2021/03/beginners-guide-to-support-vector-machine-svm/#:~:text=A%20hyperplane%20is%20a%20decision,input%20features%20in%20the%20dataset.\n",
    "\n",
    "\n",
    "\n",
    "15. Pupale. R. Support Vector Machines(SVM) — An Overview(2018) https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989\n",
    "\n",
    "\n",
    "16. Ahmed.T. et al. (2020). Analysis of Wisconsin Breast Cancer original dataset using data mining and machine learning algorithms for breast cancer prediction. https://www.journalbinet.com/uploads/2/1/0/0/21005390/67.02.09.2020_analysis_of_wisconsin_breast_cancer_original_dataset_using_data_mining_and_machine_learning_algorithms_for_breast_cancer_prediction.pdf\n",
    "\n",
    "\n",
    "17. Educative, Inc. What is a multi-layered perceptron?. (2022). https://www.educative.io/answers/what-is-a-multi-layered-perceptron\n",
    "\n",
    "\n",
    "18. Khanna.N. J48 Classification (C4.5 Algorithm) in a Nutshell. https://medium.com/@nilimakhanna1/j48-classification-c4-5-algorithm-in-a-nutshell-24c50d20658e\n",
    "\n",
    "\n",
    "19. G. Salama et al. (2012). Experimental Comparison of Classifiers for Breast Cancer Diagnosis. https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6408508 , https://www.semanticscholar.org/paper/Experimental-comparison-of-classifiers-for-breast-Salama-Abdelhalim/1437a72d1445abed6ae51020d0d041c63ef57bf3\n",
    "\n",
    "\n",
    "20. Arach. S,  Bouden. H (2019). Performance Analysis on Three Breast Cancer Datasets using Ensemble Classifiers Techniques http://ijmcs.future-in-tech.net/14.4/R-Arach.pdf\n",
    "\n",
    "\n",
    "21. Wikipedia. Random forest (2022). https://en.wikipedia.org/wiki/Random_forest\n",
    "\n",
    "\n",
    "\n",
    "22. Zacharski. R. Training Sets, Test Sets, and 10-fold Cross-validation. (2022). https://www.kdnuggets.com/2018/01/training-test-sets-cross-validation.html\n",
    "\n",
    "\n",
    "23. The scikit-yb developers. Recursive Feature Elimination. (2019). https://www.scikit-yb.org/en/latest/api/model_selection/rfecv.html\n",
    "\n",
    "\n",
    "24. Al-Joda. A.A, et al. (2021). Comparison of classification techniques based on medical datasets. https://ijnaa.semnan.ac.ir/article_5965_a1ca78998e8760f2915afc8b1d037e64.pdf\n",
    "\n",
    "\n",
    "25. Brownlee. J. Boosting and AdaBoost for Machine Learning. (2016). https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/\n",
    "\n",
    "\n",
    "26. Abdulkareema S.A and Abdulkareem Z.O.(2021). An Evaluation of the Wisconsin Breast Cancer Dataset using Ensemble Classifiers and RFE Feature Selection Technique file:///C:/Users/Admin/Downloads/monther,+12300-Article+Text-36825-1-6-20210211%20(1).pdf\n",
    "\n",
    "\n",
    "27. Bhandari. A. Analytics Vidhya. Everything you Should Know about Confusion Matrix for Machine Learning. (2022). https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/\n",
    "\n",
    "\n",
    "28. M. Amrane, S. Oukid, I. Gagaoua and T. Ensarİ, \"Breast cancer classification using machine learning,\" 2018 Electric Electronics, Computer Science, Biomedical Engineerings' Meeting (EBBT), 2018, pp. 1-4, doi: 10.1109/EBBT.2018.8391453. https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8391453\n",
    "\n",
    "\n",
    "29. Wikipedia. Euclidean distance. (2022). https://en.wikipedia.org/wiki/Euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf95b9c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742c8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
